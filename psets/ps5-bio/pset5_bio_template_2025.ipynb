{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlOtIehkfv94"
   },
   "source": [
    "# Problem Set 5\n",
    "<center> 7.C01/7.C51, 20.C01/20.C51<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-ST-K4gfv98"
   },
   "source": [
    "<b>Name:</b>\n",
    "\n",
    "<b>Kerberos id:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRjyJW47RVfB"
   },
   "source": [
    "### Download required data & install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgfwXqRsRJKN"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/main/psets/ps5-bio/data/luxA_train.csv\n",
    "!wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/main/psets/ps5-bio/data/luxA_val.csv\n",
    "!wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/main/psets/ps5-bio/data/luxA_test.csv\n",
    "!wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/main/psets/ps2-bio/data/dna_binding.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iy35fXcR-ZgA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import random as r\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbgK3PWfWcNz"
   },
   "source": [
    "## Part 1: Variational auto-encoders (VAEs) for proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "og7w9awYfQV-"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "luxA_train = pd.read_csv(\"luxA_train.csv\")\n",
    "luxA_val = pd.read_csv(\"luxA_val.csv\")\n",
    "luxA_test = pd.read_csv(\"luxA_test.csv\")\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sz_aXbb-fa4A"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "# character list for FASTA protein sequences\n",
    "aa_charset = ['-', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K',\n",
    "              'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "\n",
    "# define encoder\n",
    "enc = preprocessing.LabelEncoder().fit(aa_charset)\n",
    "\n",
    "# determine max length of protein sequences (already padded to have same length)\n",
    "max_len = len(luxA_train.aa_sequence[0]) # 360\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bqpmu-99ftw6"
   },
   "source": [
    "### 1.1 (5 points) Encode protein sequences into numerical vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFHN_-Awf1Ic"
   },
   "source": [
    "Encode FASTA sequences into categorical vectors to make train/validation/test Datasets and DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jFwBUD9f20z"
   },
   "outputs": [],
   "source": [
    "################ Solution #################\n",
    "\n",
    "X_train = None\n",
    "X_val = None\n",
    "X_test = None\n",
    "\n",
    "batch_size = None\n",
    "\n",
    "train_data = None\n",
    "train_loader = None\n",
    "\n",
    "val_data = None\n",
    "val_loader = None\n",
    "\n",
    "test_data = None\n",
    "test_loader = None\n",
    "\n",
    "################ Solution #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef-O5LVLgDk7"
   },
   "source": [
    "### 1.2 (15 points) Implement the reparametrization trick for VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xsh1xUSXgH9W"
   },
   "outputs": [],
   "source": [
    "class ProteinVAE(nn.Module):\n",
    "    def __init__(self, rnn_enc_hid_dim, enc_nconv, encoder_hid, z_dim,\n",
    "                 rnn_dec_hid_dim, dec_nconv, aa_seq_len, nchar):\n",
    "        super(ProteinVAE, self).__init__()\n",
    "        \"\"\"\n",
    "            Protein VAE model\n",
    "\n",
    "                rnn_enc_hid_dim: hidden dimension for the GRU encoder\n",
    "                enc_nconv: number of recurrent layers for the GRU decoder\n",
    "                encoder_hid: dimension of GRU encoder readout\n",
    "                z_dim: number of latent variable\n",
    "                rnn_dec_hid_dim: hidden dimension for the GRU decoder\n",
    "                dec_nconv: number of recurrent layers for the GRU decoder\n",
    "                aa_seq_len: total length of aligned amino acid sequences\n",
    "                nchar: number of possible characters\n",
    "        \"\"\"\n",
    "        self.aa_seq_len = aa_seq_len\n",
    "        self.nchar = nchar\n",
    "\n",
    "        self.embed = nn.Embedding(self.nchar, rnn_enc_hid_dim)  # embedding layer\n",
    "        self.rnn_enc = nn.GRU(rnn_enc_hid_dim, rnn_enc_hid_dim,\n",
    "                              enc_nconv, batch_first=True)  # encoding GRU\n",
    "        self.mlp0 = nn.Linear(rnn_enc_hid_dim, encoder_hid)  # transfrom hidden from encoding GRU\n",
    "        self.mu_network = nn.Linear(encoder_hid, z_dim)  # to parametrize mu\n",
    "        self.logvar_network = nn.Linear(encoder_hid, z_dim)  # to parametrize log variance\n",
    "        self.rnn_dec = nn.GRU(z_dim, rnn_dec_hid_dim, dec_nconv,\n",
    "                              batch_first=True)  # decoding GRU\n",
    "        self.readout = nn.Linear(rnn_dec_hid_dim, self.nchar)  # output characters\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\" Output mean and log variance of the encoded sequence\n",
    "        \"\"\"\n",
    "        output, hn = self.rnn_enc(x)\n",
    "        h = torch.nn.functional.relu(self.mlp0(hn[-1]))\n",
    "\n",
    "        return self.mu_network(h), self.logvar_network(h)\n",
    "\n",
    "    def get_std(self, logvar):\n",
    "        \"\"\" Transform log variance to standard deviation\n",
    "        \"\"\"\n",
    "        ################ Solution #################\n",
    "\n",
    "        std = None\n",
    "\n",
    "        ################ Solution #################\n",
    "        return std\n",
    "\n",
    "    def reparametrize(self, mu, std):\n",
    "        \"\"\" The reparametrization trick\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            ################ Solution #################\n",
    "\n",
    "            z = None\n",
    "\n",
    "            ################ Solution #################\n",
    "            return z\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\" Decoder to reconstruct latent variable back to sequence\n",
    "        \"\"\"\n",
    "        z = z.view(z.size(0), 1, z.size(-1)).repeat(1, self.aa_seq_len, 1)\n",
    "        out, h = self.rnn_dec(z)\n",
    "        out_reshape = out.contiguous().view(-1, out.size(-1))\n",
    "\n",
    "        y0 = self.readout(out_reshape)\n",
    "        y = y0.contiguous().view(out.size(0), -1, y0.size(-1))\n",
    "\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embed(x)  # get sequence embedding\n",
    "        mu, logvar = self.encode(x_embed)  # encoding sequence to latent\n",
    "        std = self.get_std(logvar)  # transfrom log variance to std\n",
    "\n",
    "        z = self.reparametrize(mu, std)  # reparametrization trick\n",
    "        sequence_recon = self.decode(z)  # reconstruct sequence string\n",
    "\n",
    "        return sequence_recon, mu, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXKghZergRoG"
   },
   "source": [
    "Test your model by comparing your sampling with N(0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1PRPM9PgSm8"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "# define your model\n",
    "model = ProteinVAE(rnn_enc_hid_dim=256, enc_nconv=1, encoder_hid=256,\n",
    "                   z_dim=128, rnn_dec_hid_dim=512, dec_nconv=3,\n",
    "                   nchar=len(aa_charset), aa_seq_len=max_len)\n",
    "\n",
    "# compare your sampling with N(0, 1)\n",
    "sample = model.reparametrize(torch.zeros(1000), torch.ones(1000))\n",
    "plt.hist(sample.detach().cpu().numpy(), density=True)\n",
    "\n",
    "# plot between -10 and 10 with .001 steps\n",
    "x_axis = np.arange(-7, 7, 0.001)\n",
    "plt.plot(x_axis, norm.pdf(x_axis,0,1))  # mean = 0, std = 1\n",
    "plt.show()\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCeBdzgzgfaY"
   },
   "source": [
    "### 1.3 (10 points) Implement the protein VAE loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42rKFESHgoUd"
   },
   "source": [
    "Implement your loss function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TFg3peigo9G"
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, std):\n",
    "    ################ Solution #################\n",
    "\n",
    "    BCE = None\n",
    "    KLD = None\n",
    "\n",
    "    ################ Solution #################\n",
    "    return BCE, KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4mIj0k-g0J5"
   },
   "source": [
    "### 1.4 (10 points) Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ywl0_T_djRDA"
   },
   "source": [
    "Simply run the following code chunks to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szoR9SJ7hGOt"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "def loop(model, loader, epoch, beta=0.05, evaluation=False):\n",
    "    \"\"\" Train/test your VAE model\n",
    "    \"\"\"\n",
    "    if evaluation:\n",
    "        model.eval()\n",
    "        mode = \"eval\"\n",
    "    else:\n",
    "        model.train()\n",
    "        mode = \"train\"\n",
    "    batch_losses = []\n",
    "\n",
    "    tqdm_data = tqdm(loader, position=0, leave=True, desc=f\"{mode} (epoch #{epoch})\")\n",
    "    for data in tqdm_data:\n",
    "        x = data[0].to(device)\n",
    "        recon_batch, mu, std = model(x)\n",
    "        loss_recon, loss_kl = loss_function(recon_batch, x, mu, std)\n",
    "        loss = loss_recon + beta * loss_kl\n",
    "\n",
    "        if not evaluation:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "        postfix = [f\"recon loss={loss_recon.item():.3f}\",\n",
    "                   f\"KL loss={loss_kl.item():.3f}\",\n",
    "                   f\"total loss={loss.item():.3f}\",\n",
    "                   f\"avg. loss={np.array(batch_losses).mean():.3f}\"]\n",
    "\n",
    "        tqdm_data.set_postfix_str(\" \".join(postfix))\n",
    "\n",
    "    return np.array(batch_losses).mean()\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ug_nXmf7hGYw"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "device = 0\n",
    "model = ProteinVAE(rnn_enc_hid_dim=256, enc_nconv=1, encoder_hid=256,\n",
    "                   z_dim=128, rnn_dec_hid_dim=512, dec_nconv=3,\n",
    "                   nchar=len(aa_charset), aa_seq_len=max_len)\n",
    "model = model.to(device)\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJBJwWj9hGi5"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", factor=0.5, patience=5)\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjnSkWkHhGtz"
   },
   "source": [
    "Mount your Google Drive to save your model and files (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sly-begzhI19"
   },
   "outputs": [],
   "source": [
    "################# Run (optional) #################\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "mydrive = \"/content/drive/MyDrive\"\n",
    "\n",
    "################ Run (optional) #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6C1sf8pXhI_d"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "epochs = 50  # this may take a bit...\n",
    "for epoch in range(0, epochs):\n",
    "\n",
    "    train_loss = loop(model, train_loader, epoch, 0.001)\n",
    "    val_loss = loop(model, val_loader, epoch, 0.001,  evaluation=True)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # uncomment to save model (optional)\n",
    "    # if epoch % 15 == 0:\n",
    "    #     torch.save(model.state_dict(), f\"{mydrive}/vae-{epoch:03d}-{train_loss:.2f}.pth\")\n",
    "    #     torch.save(optimizer.state_dict(), f\"{mydrive}/optim-{epoch:03d}-{train_loss:.2f}.pth\")\n",
    "\n",
    "    if epoch == 0:\n",
    "        best_loss = train_loss.item()\n",
    "    else:\n",
    "        if train_loss.item() < best_loss:\n",
    "            best_loss = train_loss.item()\n",
    "    print(best_loss)\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9sxvuHfhLZv"
   },
   "source": [
    "### 1.5 (25 points, Grad only) Sample new protein sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsBDUbRHhOEk"
   },
   "source": [
    "A helper function for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afjWuBnShOMX"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "def index2fasta(mol_index, enc):\n",
    "    \"\"\" Transform your array of character indices back to FASTA\n",
    "    \"\"\"\n",
    "    fasta_charlist = enc.inverse_transform(np.array(mol_index))\n",
    "    fasta = \"\".join(fasta_charlist).strip(\" \")\n",
    "\n",
    "    return fasta\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks6LQtBkhQVm"
   },
   "source": [
    "Randomly select two FASTA sequences in your test data, interpolate 10 points between them, and decode those points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHXVUZPHhQmv"
   },
   "outputs": [],
   "source": [
    "################ Solution #################\n",
    "\n",
    "# select a starting and ending protein sequence\n",
    "start = index2fasta(test_loader.dataset.__getitem__(random.choices(range(len(test_loader.dataset)), k=1))[0].numpy().reshape(-1), enc)\n",
    "end = index2fasta(test_loader.dataset.__getitem__(random.choices(range(len(test_loader.dataset)), k=1))[0].numpy().reshape(-1), enc)\n",
    "model.eval()\n",
    "\n",
    "# add your code here\n",
    "\n",
    "################ Solution #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipDvEQrChTqe"
   },
   "source": [
    "Produce a scatter plot with the first two dimensions of $z$ of your test molecules and newly sampled molecules in the same figure. Color differently the test points and generated points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDCW5JZYhTzt"
   },
   "outputs": [],
   "source": [
    "################ Solution #################\n",
    "\n",
    "\n",
    "\n",
    "################ Solution #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7pwjPCuhT8E"
   },
   "source": [
    "In a separate table, print the 10 different FASTA sequences you generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kcETvAIhUEu"
   },
   "outputs": [],
   "source": [
    "################ Solution #################\n",
    "\n",
    "\n",
    "\n",
    "################ Solution #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr6vb2RRhXun"
   },
   "source": [
    "Briefly comment on the sequence diversity you observe among these 10 generated sequences. How many are essentially identical? What do you think can be done to improve the model in its ability to finely interpolate sequences in this learned latent space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLcwAQWGhX2O"
   },
   "outputs": [],
   "source": [
    "################ Solution #################\n",
    "\n",
    "\"\"\"\n",
    "asdf\n",
    "\"\"\"\n",
    "\n",
    "################ Solution #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVAkamN4fv99"
   },
   "source": [
    "## Part 2: Predicting DNA binding sites with transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNp2YOQOiioq"
   },
   "source": [
    "Load the ChIP-seq dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiP9YQbAfv-B"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "df = pd.read_csv(\"./dna_binding.csv\")\n",
    "\n",
    "sequences = df.seq.values\n",
    "y = df.bind.values\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGGr1meefv-F"
   },
   "source": [
    "Build Datasets and DataLoaders in PyTorch (from Problem Set 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xeLtJMYfv-G"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "def SeqEnc(sequences):\n",
    "    '''\n",
    "    A function to one-hot encode DNA sequences\n",
    "\n",
    "    Args:\n",
    "        sequences (list): list of DNA sequences\n",
    "\n",
    "    Returns:\n",
    "        np.array: array with shape (N,C,4) where N is the number of sequences\n",
    "        and C is the sequence length\n",
    "    '''\n",
    "\n",
    "    X = []\n",
    "    base_dict = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "\n",
    "    for seq in sequences:\n",
    "        onehot = []\n",
    "        for base in seq:\n",
    "            vec = np.zeros(4)\n",
    "            vec[base_dict[base]] = 1\n",
    "            onehot.append(vec)\n",
    "        X.append(np.array(onehot))\n",
    "\n",
    "    return np.array( X )\n",
    "\n",
    "X = SeqEnc(sequences)\n",
    "print(\"Shape of X is {}.\".format(X.shape))\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fAAi_9mfv-H"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "# generate dataset\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.Tensor(np.array(X))  # store X as a pytorch Tensor\n",
    "        self.y = torch.Tensor(np.array(y))  # store y as a pytorch Tensor\n",
    "        self.len=len(self.X)                # number of samples in the data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDcFKq7-fv-J"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, random_state=42)\n",
    "\n",
    "# define dataset\n",
    "train_data = SequenceDataset(X_train, y_train)\n",
    "val_data = SequenceDataset(X_val, y_val)\n",
    "test_data = SequenceDataset(X_test, y_test)\n",
    "\n",
    "# train/test split\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXqpwHtMfv-P"
   },
   "source": [
    "Implement functions for training and testing  (from Problem Set 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwcE6igLfv-P"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "\n",
    "    '''\n",
    "    A function to train on the entire dataset for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Your sequence classifier\n",
    "        dataloader (torch.utils.data.Dataloader): DataLoader object for the train data\n",
    "        optimizer (torch.optim.Optimizer): Optimizer object to interface gradient calculation and optimization\n",
    "        device (str): Your device\n",
    "\n",
    "    Returns:\n",
    "        float: loss averaged over all the batches\n",
    "\n",
    "    '''\n",
    "\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "\n",
    "    for batch in dataloader:\n",
    "        seq, label  = batch\n",
    "        seq = seq.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        proba =  model(seq)\n",
    "\n",
    "        loss = F.binary_cross_entropy(proba.squeeze(),label)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.array(epoch_loss).mean()\n",
    "\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "\n",
    "    '''\n",
    "    A function to validate on the validation dataset for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Your sequence classifier\n",
    "        dataloader (torch.utils.data.Dataloader): DataLoader object for the validation data\n",
    "        device (str): Your device\n",
    "\n",
    "    Returns:\n",
    "        float: loss averaged over all the batches\n",
    "\n",
    "    '''\n",
    "\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "\n",
    "            seq, label  = batch\n",
    "            seq = seq.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            proba = model(seq)\n",
    "            loss = F.binary_cross_entropy(proba.squeeze(),label)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "        return np.array(val_loss).mean()\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "\n",
    "    '''\n",
    "    A function to return the classification probabilities and true labels (for evaluation).\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): your sequence classifier\n",
    "        dataloader (torch.utils.data.Dataloader): DataLoader object for the train data\n",
    "        device (str): Your device\n",
    "\n",
    "    Returns:\n",
    "        (np.array, np.array): true labels, predicted probabilities\n",
    "    '''\n",
    "\n",
    "    pred_prob = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in dataloader:\n",
    "            epoch_loss = []\n",
    "            seq, label = batch\n",
    "\n",
    "            seq = seq.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            proba = model(seq)\n",
    "            batch_pred=proba.squeeze().cpu().detach().numpy().tolist()\n",
    "            batch_labels=label.cpu().numpy().squeeze().tolist()\n",
    "\n",
    "            labels += batch_labels\n",
    "            pred_prob += batch_pred\n",
    "\n",
    "    return labels, pred_prob\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfac5z9VVrNd"
   },
   "source": [
    "### 2.1 (15 points) Implement a transformer encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrZQNTluVx42"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\" Defines positional encoding (adapted from PyTorch's\n",
    "        documentation)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(101).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0)/d_model))\n",
    "        pe = torch.zeros(1, 101, d_model)\n",
    "        pe[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x += self.pe\n",
    "        return self.dropout(x)\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYa1ntzgi5uG"
   },
   "outputs": [],
   "source": [
    "class TransformerSeq(nn.Module):\n",
    "    \"\"\" Defines DNA sequence transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead, num_layers, positional=True):\n",
    "        super(TransformerSeq, self).__init__()\n",
    "        self.positional = positional\n",
    "\n",
    "        # to prep transformer input\n",
    "        self.in_full = nn.Linear(4, d_model)\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "\n",
    "        ################ Solution #################\n",
    "\n",
    "        # transformer encoder\n",
    "        self.layer =\n",
    "        self.encoder =\n",
    "\n",
    "        ################ Solution #################\n",
    "\n",
    "        # to output probability\n",
    "        self.out_full = nn.Linear(d_model, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply embedding and positional encoding\n",
    "        if self.positional:\n",
    "            x_in = self.pe(self.in_full(x))\n",
    "        else:\n",
    "            x_in = self.in_full(x)\n",
    "\n",
    "        ################ Solution #################\n",
    "\n",
    "        # apply transformer encoder and pool output\n",
    "        x_out =\n",
    "        pooled =\n",
    "\n",
    "        # get probability\n",
    "        prob =\n",
    "\n",
    "        ################ Solution #################\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2Cgap5dVyim"
   },
   "source": [
    "### 2.2 (20 points) Explore how positional encodings improve classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4SU1pH_jCvL"
   },
   "source": [
    "Try training the transformer with/without positional encodings! Run both models for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fE0-zkOafv-Q"
   },
   "outputs": [],
   "source": [
    "device = 0\n",
    "\n",
    "# model with positional encodings off\n",
    "model_off = TransformerSeq(16, 4, 2, positional=False).to(device)\n",
    "optimizer_off = torch.optim.Adam(model_off.parameters(), lr=0.001)\n",
    "\n",
    "################ Solution #################\n",
    "\n",
    "# model with positional encodings on\n",
    "model_on =\n",
    "optimizer_on =\n",
    "\n",
    "################ Solution #################\n",
    "\n",
    "# use tqdm for progress bar\n",
    "val_loss_off, val_loss_on = [], []\n",
    "train_loss_off, train_loss_on = [], []\n",
    "for epoch in tqdm(range(100), desc=\"Progress\"):\n",
    "    # compute training/validation loss for off model\n",
    "    train_loss_off.append(train(model_off, train_loader, optimizer_off, device=device))\n",
    "    val_loss_off.append(validate(model_off, val_loader, device=device))\n",
    "\n",
    "    ################ Solution #################\n",
    "\n",
    "    # compute training/validation loss for on model\n",
    "    train_loss_on.append()\n",
    "    val_loss_on.append()\n",
    "\n",
    "    ################ Solution #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc-un62Ufv-R"
   },
   "source": [
    "Plot train and validation loss functions with and without the positional encodings. The first plot has been made for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYUSvgUifv-R"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].plot(val_loss_off, label=\"Validation Loss\")\n",
    "axes[0].plot(train_loss_off, label=\"Training Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend(loc=\"upper right\")\n",
    "axes[0].set_title(\"Positional encoding OFF\")\n",
    "\n",
    "################ Solution #################\n",
    "\n",
    "\n",
    "\n",
    "################ Solution #################\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnXrh8TCjMSv"
   },
   "source": [
    "From what you know about transformers, why are positional encodings here necessary? If the two plots were identical, what would that tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66DCJPWgjM2z"
   },
   "outputs": [],
   "source": [
    "################ Solution #################\n",
    "\n",
    "\n",
    "\n",
    "################ Solution #################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Bqpmu-99ftw6",
    "tCeBdzgzgfaY",
    "f9sxvuHfhLZv"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
