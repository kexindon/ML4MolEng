{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlOtIehkfv94"
   },
   "source": [
    "#  <center> Problem Set 5 <center>\n",
    "<center> 3.C01/3.C51, 10.C01/10.C51 <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-ST-K4gfv98"
   },
   "source": [
    "<b>Name:</b>\n",
    "\n",
    "<b>Kerberos id:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRjyJW47RVfB"
   },
   "source": [
    "### Download required data & install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgfwXqRsRJKN"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/main/psets/ps5-nonbio/data/zinc_50k.csv\n",
    "!wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/main/psets/ps5-nonbio/data/vae-050-0.06.pth\n",
    "!wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/main/psets/ps2-nonbio/data/dna_binding.csv\n",
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iy35fXcR-ZgA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import random as r\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from scipy.stats import norm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPkuKCyvVun8"
   },
   "source": [
    "## Part 1: Variational auto-encoders for SMILES strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugdrX6lYXHq3"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "# character list\n",
    "moses_charset = ['2', 'o', 'C', 'I', 'O', 'H', 'n', 'N', '=', '+', '#', '-', 'c',\n",
    "                 'B', 'l', '7', 'r', 'S', 's', '4', '6', '[', '5', ']', 'F', '3',\n",
    "                 'P', '(', ')', '1', ' ']\n",
    "\n",
    "# define encoder\n",
    "enc = preprocessing.LabelEncoder().fit(moses_charset)\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(\"./zinc_50k.csv\")\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXZAUMgOXHci"
   },
   "source": [
    "### 1.1 (5 points) One-hot encode SMILES strings into padded numerical vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh6UGylLXcjj"
   },
   "source": [
    "Encode SMILES strings into padded categorical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcfqpwtWXc1B"
   },
   "outputs": [],
   "source": [
    "################ Solution #################\n",
    "\n",
    "# find out the longest SMILES string, pad, and encode\n",
    "\n",
    "\n",
    "################ Solution #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXhW5rq8Xo_B"
   },
   "source": [
    "Make train/validation/test Datasets and DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTCloYGtXpIA"
   },
   "outputs": [],
   "source": [
    "################ Solution #################\n",
    "\n",
    "X_train, X_test = None\n",
    "X_train, X_val = None\n",
    "\n",
    "train_data = None\n",
    "train_loader = None\n",
    "\n",
    "val_data = None\n",
    "val_loader = None\n",
    "\n",
    "test_data = None\n",
    "test_loader = None\n",
    "\n",
    "################ Solution #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LjqWAgKX8Tf"
   },
   "source": [
    "### 1.2 (15 points) Implement the reparametrization trick for VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhsAjUOiX9fy"
   },
   "outputs": [],
   "source": [
    "class MolVAE(nn.Module):\n",
    "    def __init__(self, rnn_enc_hid_dim, enc_nconv, encoder_hid, z_dim,\n",
    "                 rnn_dec_hid_dim, dec_nconv, smiles_len, nchar):\n",
    "        super(MolVAE, self).__init__()\n",
    "        \"\"\"\n",
    "            SMILES VAE model\n",
    "\n",
    "                rnn_enc_hid_dim: hidden dimension for the GRU encoder\n",
    "                enc_nconv: number of recurrent layers for the GRU decoder\n",
    "                encoder_hid: dimension of GUR encoder readout\n",
    "                z_dim: number of latent variable\n",
    "                rnn_dec_hid_dim: hidden dimension for the GRU decoder\n",
    "                dec_nconv: number of recurrent layers for the GRU decoder\n",
    "                smiles_len: total length of padded SMILES string\n",
    "                nchar: number of possible characters\n",
    "        \"\"\"\n",
    "        self.smiles_len = smiles_len\n",
    "        self.nchar = nchar\n",
    "\n",
    "        self.embed = nn.Embedding(self.nchar, rnn_enc_hid_dim)  # embedding layer\n",
    "        self.rnn_enc = nn.GRU(rnn_enc_hid_dim, rnn_enc_hid_dim,\n",
    "                              enc_nconv, batch_first=True)  # encoding GRU\n",
    "        self.mlp0 = nn.Linear(rnn_enc_hid_dim, encoder_hid)  # transfrom hidden from encoding GRU\n",
    "        self.mu_network = nn.Linear(encoder_hid, z_dim)  # to parametrize mu\n",
    "        self.logvar_network = nn.Linear(encoder_hid, z_dim)  # to parametrize log variance\n",
    "        self.rnn_dec = nn.GRU(z_dim, rnn_dec_hid_dim, dec_nconv,\n",
    "                              batch_first=True)  # decoding GRU\n",
    "        self.readout = nn.Linear(rnn_dec_hid_dim, self.nchar)  # output characters\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\" Output mean and log variance of the encoded SMILES\n",
    "        \"\"\"\n",
    "        output, hn = self.rnn_enc(x)\n",
    "        h = torch.nn.functional.relu(self.mlp0(hn[-1]))\n",
    "\n",
    "        return self.mu_network(h), self.logvar_network(h)\n",
    "\n",
    "    def get_std(self, logvar):\n",
    "        \"\"\" Transform log variance to standard deviation\n",
    "        \"\"\"\n",
    "        ################ Solution #################\n",
    "\n",
    "        std = None\n",
    "\n",
    "        ################ Solution #################\n",
    "        return std\n",
    "\n",
    "    def reparametrize(self, mu, std):\n",
    "        \"\"\" The reparametrization trick\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            ################ Solution #################\n",
    "\n",
    "            z = None\n",
    "\n",
    "            ################ Solution #################\n",
    "            return z\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\" Decoder to reconstruct latent variable back to SMILES\n",
    "        \"\"\"\n",
    "        z = z.view(z.size(0), 1, z.size(-1)).repeat(1, self.smiles_len, 1)\n",
    "        out, h = self.rnn_dec(z)\n",
    "        out_reshape = out.contiguous().view(-1, out.size(-1))\n",
    "\n",
    "        y0 = self.readout(out_reshape)\n",
    "        y = y0.contiguous().view(out.size(0), -1, y0.size(-1))\n",
    "\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embed(x)  # get SMILES embedding\n",
    "        mu, logvar = self.encode(x_embed)  # encoding SMILES to latent\n",
    "        std = self.get_std(logvar)  # transfrom log variance to std\n",
    "\n",
    "        z = self.reparametrize(mu, std)  # reparametrization trick\n",
    "        smiles_recon = self.decode(z)  # reconstruct SMILES string\n",
    "\n",
    "        return smiles_recon, mu, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGNqfFJKaYer"
   },
   "source": [
    "Test your model by comparing your sampling with N(0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUDTV5NFaYwm"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "# define your model\n",
    "model = MolVAE(rnn_enc_hid_dim=256, enc_nconv=1, encoder_hid=256, z_dim=128,\n",
    "               rnn_dec_hid_dim=512, dec_nconv=3, nchar=31, smiles_len=max_len)\n",
    "\n",
    "# compare your sampling with N(0, 1)\n",
    "sample = model.reparametrize(torch.zeros(1000), torch.ones(1000))\n",
    "plt.hist(sample.detach().cpu().numpy(), density=True)\n",
    "\n",
    "# plot between -10 and 10 with .001 steps.\n",
    "x_axis = np.arange(-7, 7, 0.001)\n",
    "plt.plot(x_axis, norm.pdf(x_axis,0,1))  # mean = 0, std = 1\n",
    "plt.show()\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u40rgn9AYdkO"
   },
   "source": [
    "### 1.3 (10 points) Implement the SMILES VAE loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tlyJRa7aBqb"
   },
   "source": [
    "Implement your loss function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_M7Y7Q0aBxj"
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, std):\n",
    "    ################ Solution #################\n",
    "\n",
    "    BCE = None\n",
    "    KLD = None\n",
    "\n",
    "    ################ Solution #################\n",
    "    return BCE, KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0klA4RNYmz9"
   },
   "source": [
    "### 1.4 (10 points) Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkRJSWfbZjTC"
   },
   "source": [
    "Simply run the following code chunks to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xmKKh2rZkua"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "def loop(model, loader, epoch, beta=0.05, evaluation=False):\n",
    "    \"\"\" Train/test your VAE model\n",
    "    \"\"\"\n",
    "    if evaluation:\n",
    "        model.eval()\n",
    "        mode = \"eval\"\n",
    "    else:\n",
    "        model.train()\n",
    "        mode = \"train\"\n",
    "    batch_losses = []\n",
    "\n",
    "    tqdm_data = tqdm(loader, position=0, leave=True, desc=f\"{mode} (epoch #{epoch})\")\n",
    "    for data in tqdm_data:\n",
    "        x = data[0].to(device)\n",
    "        recon_batch, mu, std = model(x)\n",
    "        loss_recon, loss_kl = loss_function(recon_batch, x, mu, std)\n",
    "        loss = loss_recon + beta * loss_kl\n",
    "\n",
    "        if not evaluation:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "        postfix = [f\"recon loss={loss_recon.item():.3f}\",\n",
    "                   f\"KL loss={loss_kl.item():.3f}\",\n",
    "                   f\"total loss={loss.item():.3f}\",\n",
    "                   f\"avg. loss={np.array(batch_losses).mean():.3f}\"]\n",
    "\n",
    "        tqdm_data.set_postfix_str(\" \".join(postfix))\n",
    "\n",
    "    return np.array(batch_losses).mean()\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0d5-NrXqZnTI"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "device = 0\n",
    "model = MolVAE(rnn_enc_hid_dim=367, enc_nconv=2, encoder_hid=512, z_dim=171,\n",
    "               rnn_dec_hid_dim=512, dec_nconv=1, nchar=31, smiles_len=max_len)\n",
    "model = model.to(device)\n",
    "\n",
    "# load pretrained model\n",
    "model.load_state_dict(torch.load(\"./vae-050-0.06.pth\"))\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDU9-b-nZnc0"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", factor=0.5, patience=5)\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v55FOHIbzXI"
   },
   "source": [
    "Mount your Google Drive to save your model and files (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N13c2kVJb0yY"
   },
   "outputs": [],
   "source": [
    "################# Run (optional) #################\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "mydrive = \"/content/drive/MyDrive\"\n",
    "\n",
    "################ Run (optional) #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eWGdECHZnkV"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(0, epochs):\n",
    "\n",
    "    train_loss = loop(model, train_loader, epoch, 0.001)\n",
    "    val_loss = loop(model, val_loader, epoch, 0.001,  evaluation=True)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # uncomment to save model (optional)\n",
    "    # if epoch % 15 == 0:\n",
    "    #     torch.save(model.state_dict(), f\"{mydrive}/vae-{epoch:03d}-{train_loss:.2f}.pth\")\n",
    "    #     torch.save(optimizer.state_dict(), f\"{mydrive}/optim-{epoch:03d}-{train_loss:.2f}.pth\")\n",
    "\n",
    "    if epoch == 0:\n",
    "        best_loss = train_loss.item()\n",
    "    else:\n",
    "        if train_loss.item() < best_loss:\n",
    "            best_loss = train_loss.item()\n",
    "    print(best_loss)\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9q2gZbLhYssk"
   },
   "source": [
    "### 1.5 (25 points, Grad only) Sample new molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ci2CrXlFY3cl"
   },
   "source": [
    "Some helper functions for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSw5Q4SdY4Jm"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "def index2smiles(mol_index, enc):\n",
    "    \"\"\" Transform your array of character indices back to SMILES\n",
    "    \"\"\"\n",
    "    smiles_charlist = enc.inverse_transform(np.array(mol_index))\n",
    "    smiles = \"\".join(smiles_charlist).strip(\" \")\n",
    "\n",
    "    return smiles\n",
    "\n",
    "def check_smiles_valid(smiles):\n",
    "    \"\"\" Check if SMILES string is valid\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        valid = True\n",
    "    else:\n",
    "        valid = False\n",
    "    return valid\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20foEHySZIab"
   },
   "source": [
    "Randomly select two SMILES in your test data, interpolate 10 points between them, and decode those points. Test them for accuracy and draw the scatter plot of the lower 2 dimensions. Then visualize any molecules that worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvf41Ld4ZIoY"
   },
   "outputs": [],
   "source": [
    "################ Solution #################\n",
    "\n",
    "# select a starting and ending molecule\n",
    "start = index2smiles(test_loader.dataset.__getitem__(r.choices(range(len(test_loader.dataset)), k=1))[0].numpy().reshape(-1), enc)\n",
    "end = index2smiles(test_loader.dataset.__getitem__(r.choices(range(len(test_loader.dataset)), k=1))[0].numpy().reshape(-1), enc)\n",
    "model.eval()\n",
    "\n",
    "################ Solution #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3l_zytiaRfN"
   },
   "source": [
    "Produce a scatter plot with the first two dimensions of $z$ of your test molecules and newly sampled molecules in the same figure. Color differently the test points and generated points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20oHA_r2ZLkh"
   },
   "outputs": [],
   "source": [
    "################ Solution #################\n",
    "\n",
    "\n",
    "\n",
    "################ Solution #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfKF6MxgaKSN"
   },
   "source": [
    "Draw different molecules you generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5hSFT3UZLrA"
   },
   "outputs": [],
   "source": [
    "################ Solution #################\n",
    "\n",
    "\n",
    "\n",
    "################ Solution #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-VBju0-ZOPM"
   },
   "source": [
    "Why does the VAE sometimes fail to generate valid SMILES strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knll91dDZSby"
   },
   "outputs": [],
   "source": [
    "################ Solution #################\n",
    "\n",
    "\n",
    "\n",
    "################ Solution #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVAkamN4fv99"
   },
   "source": [
    "## Part 2: Predicting DNA binding sites with transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2d47HEylKMr"
   },
   "source": [
    "Load the ChIP-seq dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_O5BlybxlKaz"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "df = pd.read_csv(\"./dna_binding.csv\")\n",
    "\n",
    "sequences = df.seq.values\n",
    "y = df.bind.values\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9ppANSulKpL"
   },
   "source": [
    "Build Datasets and DataLoaders in PyTorch (from Problem Set 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzciOyrBlK7I"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "def SeqEnc(sequences):\n",
    "    '''\n",
    "    A function to one-hot encode DNA sequences\n",
    "\n",
    "    Args:\n",
    "        sequences (list): list of DNA sequences\n",
    "\n",
    "    Returns:\n",
    "        np.array: array with shape (N,C,4) where N is the number of sequences\n",
    "        and C is the sequence length\n",
    "    '''\n",
    "\n",
    "    X = []\n",
    "    base_dict = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "\n",
    "    for seq in sequences:\n",
    "        onehot = []\n",
    "        for base in seq:\n",
    "            vec = np.zeros(4)\n",
    "            vec[base_dict[base]] = 1\n",
    "            onehot.append(vec)\n",
    "        X.append(np.array(onehot))\n",
    "\n",
    "    return np.array( X )\n",
    "\n",
    "X = SeqEnc(sequences)\n",
    "print(\"Shape of X is {}.\".format(X.shape))\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYaZniYAlLjh"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "# generate dataset\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.Tensor(np.array(X))  # store X as a pytorch Tensor\n",
    "        self.y = torch.Tensor(np.array(y))  # store y as a pytorch Tensor\n",
    "        self.len=len(self.X)                # number of samples in the data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEp2aiYnlLss"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, random_state=42)\n",
    "\n",
    "# define dataset\n",
    "train_data = SequenceDataset(X_train, y_train)\n",
    "val_data = SequenceDataset(X_val, y_val)\n",
    "test_data = SequenceDataset(X_test, y_test)\n",
    "\n",
    "# train/test split\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ILXHTU0lMGQ"
   },
   "source": [
    "Implement functions for training and testing (from Problem Set 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0VbrYNWlMQt"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "\n",
    "    '''\n",
    "    A function to train on the entire dataset for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Your sequence classifier\n",
    "        dataloader (torch.utils.data.Dataloader): DataLoader object for the train data\n",
    "        optimizer (torch.optim.Optimizer): Optimizer object to interface gradient calculation and optimization\n",
    "        device (str): Your device\n",
    "\n",
    "    Returns:\n",
    "        float: loss averaged over all the batches\n",
    "\n",
    "    '''\n",
    "\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "\n",
    "    for batch in dataloader:\n",
    "        seq, label  = batch\n",
    "        seq = seq.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        proba =  model(seq)\n",
    "\n",
    "        loss = F.binary_cross_entropy(proba.squeeze(),label)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.array(epoch_loss).mean()\n",
    "\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "\n",
    "    '''\n",
    "    A function to validate on the validation dataset for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Your sequence classifier\n",
    "        dataloader (torch.utils.data.Dataloader): DataLoader object for the validation data\n",
    "        device (str): Your device\n",
    "\n",
    "    Returns:\n",
    "        float: loss averaged over all the batches\n",
    "\n",
    "    '''\n",
    "\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "\n",
    "            seq, label  = batch\n",
    "            seq = seq.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            proba = model(seq)\n",
    "            loss = F.binary_cross_entropy(proba.squeeze(),label)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "        return np.array(val_loss).mean()\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "\n",
    "    '''\n",
    "    A function to return the classification probabilities and true labels (for evaluation).\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): your sequence classifier\n",
    "        dataloader (torch.utils.data.Dataloader): DataLoader object for the train data\n",
    "        device (str): Your device\n",
    "\n",
    "    Returns:\n",
    "        (np.array, np.array): true labels, predicted probabilities\n",
    "    '''\n",
    "\n",
    "    pred_prob = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in dataloader:\n",
    "            epoch_loss = []\n",
    "            seq, label = batch\n",
    "\n",
    "            seq = seq.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            proba = model(seq)\n",
    "            batch_pred=proba.squeeze().cpu().detach().numpy().tolist()\n",
    "            batch_labels=label.cpu().numpy().squeeze().tolist()\n",
    "\n",
    "            labels += batch_labels\n",
    "            pred_prob += batch_pred\n",
    "\n",
    "    return labels, pred_prob\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOs4z6nWlhZv"
   },
   "source": [
    "### 2.1 (15 points) Implement a transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybspyU8tlhxC"
   },
   "outputs": [],
   "source": [
    "################ Run #################\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\" Defines positional encoding (adapted from PyTorch's\n",
    "        documentation)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(101).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0)/d_model))\n",
    "        pe = torch.zeros(1, 101, d_model)\n",
    "        pe[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x += self.pe\n",
    "        return self.dropout(x)\n",
    "\n",
    "################ Run #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTun5GNylh48"
   },
   "outputs": [],
   "source": [
    "class TransformerSeq(nn.Module):\n",
    "    \"\"\" Defines DNA sequence transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead, num_layers, positional=True):\n",
    "        super(TransformerSeq, self).__init__()\n",
    "        self.positional = positional\n",
    "\n",
    "        # to prep transformer input\n",
    "        self.in_full = nn.Linear(4, d_model)\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "\n",
    "        ################ Solution #################\n",
    "\n",
    "        # transformer encoder\n",
    "        self.layer =\n",
    "        self.encoder =\n",
    "\n",
    "        ################ Solution #################\n",
    "\n",
    "        # to output probability\n",
    "        self.out_full = nn.Linear(d_model, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply embedding and positional encoding\n",
    "        if self.positional:\n",
    "            x_in = self.pe(self.in_full(x))\n",
    "        else:\n",
    "            x_in = self.in_full(x)\n",
    "\n",
    "        ################ Solution #################\n",
    "\n",
    "        # apply transformer encoder and pool output\n",
    "        x_out =\n",
    "        pooled =\n",
    "\n",
    "        # get probability\n",
    "        prob =\n",
    "\n",
    "        ################ Solution #################\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfopTolSlipP"
   },
   "source": [
    "### 2.2 (20 points) Explore how positional encodings improve classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HGGv_gdliyt"
   },
   "source": [
    "Try training the transformer with/without positional encodings! Run both models for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDB9IaREljU1"
   },
   "outputs": [],
   "source": [
    "device = 0\n",
    "\n",
    "# model with positional encodings off\n",
    "model_off = TransformerSeq(16, 4, 2, positional=False).to(device)\n",
    "optimizer_off = torch.optim.Adam(model_off.parameters(), lr=0.001)\n",
    "\n",
    "################ Solution #################\n",
    "\n",
    "# model with positional encodings on\n",
    "model_on =\n",
    "optimizer_on =\n",
    "\n",
    "################ Solution #################\n",
    "\n",
    "# use tqdm for progress bar\n",
    "val_loss_off, val_loss_on = [], []\n",
    "train_loss_off, train_loss_on = [], []\n",
    "for epoch in tqdm(range(100), desc=\"Progress\"):\n",
    "    # compute training/validation loss for off model\n",
    "    train_loss_off.append(train(model_off, train_loader, optimizer_off, device=device))\n",
    "    val_loss_off.append(validate(model_off, val_loader, device=device))\n",
    "\n",
    "    ################ Solution #################\n",
    "\n",
    "    # compute training/validation loss for on model\n",
    "    train_loss_on.append()\n",
    "    val_loss_on.append()\n",
    "\n",
    "    ################ Solution #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_YRUA_iljeE"
   },
   "source": [
    "Plot train and validation loss functions with and without the positional encodings. The first plot has been made for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiDKR3IiljnM"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].plot(val_loss_off, label=\"Validation Loss\")\n",
    "axes[0].plot(train_loss_off, label=\"Training Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend(loc=\"upper right\")\n",
    "axes[0].set_title(\"Positional encoding OFF\")\n",
    "\n",
    "################ Solution #################\n",
    "\n",
    "\n",
    "\n",
    "################ Solution #################\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H9B9mCQlkJw"
   },
   "source": [
    "From what you know about transformers, why are positional encodings here necessary? If the two plots were identical, what would that tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlLHFoEXlkRD"
   },
   "outputs": [],
   "source": [
    "################ Solution #################\n",
    "\n",
    "\n",
    "\n",
    "################ Solution #################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
